{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification with the Longformer\n",
    "\n",
    "In a previous post I explored the functionality of the Longformer for text classification. In this post I will explore the performance of the Longformer in a setting of multilabel classification problem.\n",
    "\n",
    "For this dataset we need to download it manually from Kaggle and load it like we usually do with the datasets library. 'jigsaw_toxicity_pred', data_dir='/path/to/extracted/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import LongformerTokenizerFast, LongformerModel, LongformerConfig\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel, LongformerClassificationHead\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to instantiate a raw LongFormer Model and add a classifier head on top. \n",
    "\n",
    "talk about pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe\n",
    "insults = pd.read_csv('../data/jigsaw/train.csv')\n",
    "insults['labels'] = insults[insults.columns[2:]].values.tolist()\n",
    "insults = insults[['id','comment_text', 'labels']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "insults_train, insults_test = train_test_split(insults,\n",
    "                                               random_state = 55,\n",
    "                                               test_size = 0.35)\n",
    "insults_test.head()\n",
    "insults_test.columns\n",
    "'''\n",
    "train_size = 0.8\n",
    "train_dataset=insults.sample(frac=train_size,random_state=200)\n",
    "test_dataset=insults.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6725d5a6391e5c77</td>\n",
       "      <td>Goal scored for Portugal \\n\\nThis could be mil...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28ea0d2c61db3137</td>\n",
       "      <td>My mistake someone was vandalizing the page so...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4de3d6b966b58ec7</td>\n",
       "      <td>Test card F music \\nCeefax music isn't the sam...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af602c4c5f1b09bc</td>\n",
       "      <td>\":Meh, I guess I can live with either outcome,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9e412a7965873237</td>\n",
       "      <td>UV is my error, above. I'm told by Kimberly Ja...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127652</th>\n",
       "      <td>c123b71e899fe446</td>\n",
       "      <td>\"\\n\\nFair use rationale for Image:Leonidas_01....</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127653</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127654</th>\n",
       "      <td>40fbd29d34b1a027</td>\n",
       "      <td>, especially kickstarter-level films, or any i...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127655</th>\n",
       "      <td>1c25899b09994ad0</td>\n",
       "      <td>Commenting on my talk page \\n\\nThose who are  ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127656</th>\n",
       "      <td>e3852fa664b03c63</td>\n",
       "      <td>Well, make it a lifetime sentence. *** ******,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127657 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       6725d5a6391e5c77  Goal scored for Portugal \\n\\nThis could be mil...   \n",
       "1       28ea0d2c61db3137  My mistake someone was vandalizing the page so...   \n",
       "2       4de3d6b966b58ec7  Test card F music \\nCeefax music isn't the sam...   \n",
       "3       af602c4c5f1b09bc  \":Meh, I guess I can live with either outcome,...   \n",
       "4       9e412a7965873237  UV is my error, above. I'm told by Kimberly Ja...   \n",
       "...                  ...                                                ...   \n",
       "127652  c123b71e899fe446  \"\\n\\nFair use rationale for Image:Leonidas_01....   \n",
       "127653  00040093b2687caa  alignment on this subject and which are contra...   \n",
       "127654  40fbd29d34b1a027  , especially kickstarter-level films, or any i...   \n",
       "127655  1c25899b09994ad0  Commenting on my talk page \\n\\nThose who are  ...   \n",
       "127656  e3852fa664b03c63  Well, make it a lifetime sentence. *** ******,...   \n",
       "\n",
       "                    labels  \n",
       "0       [0, 0, 0, 0, 0, 0]  \n",
       "1       [0, 0, 0, 0, 0, 0]  \n",
       "2       [0, 0, 0, 0, 0, 0]  \n",
       "3       [0, 0, 0, 0, 0, 0]  \n",
       "4       [0, 0, 0, 0, 0, 0]  \n",
       "...                    ...  \n",
       "127652  [0, 0, 0, 0, 0, 0]  \n",
       "127653  [0, 0, 0, 0, 0, 0]  \n",
       "127654  [0, 0, 0, 0, 0, 0]  \n",
       "127655  [0, 0, 0, 0, 0, 0]  \n",
       "127656  [0, 0, 0, 0, 0, 0]  \n",
       "\n",
       "[127657 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#insults_test = pd.read_csv('../data/jigsaw/test.csv')\n",
    "#insults_test_ids = pd.read_csv('../data/jigsaw/test_labels.csv')\n",
    "#insults_test['labels']\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a class that will handle the data\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, tokenizer, id_column, text_column, label_column):\n",
    "        \n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "    \n",
    "        # define the label column and transform it to list\n",
    "        self.label_column = label_column\n",
    "        \n",
    "        # define the id column and transform it to list\n",
    "        self.id_column = id_column.tolist()\n",
    "        \n",
    "    \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.text_column[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(comment_text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length= 2048,\n",
    "                                       padding = 'max_length',\n",
    "                                       return_attention_mask = True,\n",
    "                                       truncation = True,\n",
    "                                       return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        labels_ = torch.tensor(self.label_column[index], dtype=torch.float)\n",
    "        id_ = self.id_column[index]\n",
    "        return input_ids, attention_mask, labels_, id_\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.text_column) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "# create a class to process the traininga and test data\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', \n",
    "                                                    padding = 'max_length',\n",
    "                                                    truncation=True, \n",
    "                                                    max_length = 2048)\n",
    "training_data = Data_Processing(tokenizer, \n",
    "                                train_dataset['id'], \n",
    "                                train_dataset['comment_text'], \n",
    "                                train_dataset['labels'])\n",
    "\n",
    "test_data =  Data_Processing(tokenizer, \n",
    "                             test_dataset['id'], \n",
    "                             test_dataset['comment_text'], \n",
    "                             test_dataset['labels'])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "                    'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                 'val':len(test_data)\n",
    "                }\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[    0,   100,   524,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[    0, 39858, 46975,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[    0, 10643,   768,  ...,     1,     1,     1]],\n",
       " \n",
       "         [[    0, 14517,    36,  ...,     1,     1,     1]]]),\n",
       " tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[1, 1, 1,  ..., 0, 0, 0]]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]]),\n",
       " ('8cb75aa42b956c08',\n",
       "  'cbcbf871629fa5ae',\n",
       "  '51025f6b0c82d402',\n",
       "  'cdd4fb7e2462c8a6')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check we are getting the right output\n",
    "a = next(iter(dataloaders_dict.get('val')))\n",
    "a\n",
    "#next(iter(test_data))\n",
    "#a[2].cpu().data.numpy().shape\n",
    "#len(dataloaders_dict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Longformer for multilabel classification class\n",
    "\n",
    "class LongformerForMultiLabelSequenceClassification(LongformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    We instantiate a class of LongFormer adapted for a multilabel classification task. \n",
    "    This instance takes the pooled output of the LongFormer based model and passes it through a\n",
    "    classification head. We replace the traditional Cross Entropy loss with a BCE loss that generate probabilities\n",
    "    for all the labels that we feed into the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, pos_weight=None):\n",
    "        super(LongformerForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pos_weight = pos_weight\n",
    "        self.longformer = LongformerModel(config)\n",
    "        self.classifier = LongformerClassificationHead(config)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, \n",
    "                token_type_ids=None, position_ids=None, inputs_embeds=None, \n",
    "                labels=None):\n",
    "        \n",
    "        # create global attention on sequence, and a global attention token on the `s` token\n",
    "        # the equivalent of the CLS token on BERT models\n",
    "        if global_attention_mask is None:\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            global_attention_mask[:, 0] = 1\n",
    "        \n",
    "        # pass arguments to longformer model\n",
    "        outputs = self.longformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            global_attention_mask = global_attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids)\n",
    "        \n",
    "        # if specified the model can return a dict where each key corresponds to the output of a\n",
    "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
    "        # which will have the shape (batch_size, sequence_length, hidden_size). \n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "        \n",
    "        \n",
    "        # pass the hidden states through the classifier to obtain thee logits\n",
    "        logits = self.classifier(sequence_output)\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "            labels = labels.float()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.view(-1, self.num_labels))\n",
    "            #outputs = (loss,) + outputs\n",
    "        \n",
    "        \n",
    "        return loss, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForMultiLabelSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LongformerForMultiLabelSequenceClassification(\n",
       "  (longformer): LongformerModel(\n",
       "    (embeddings): LongformerEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): LongformerEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): LongformerLayer(\n",
       "          (attention): LongformerAttention(\n",
       "            (self): LongformerSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): LongformerSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): LongformerIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): LongformerOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): LongformerPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): LongformerClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LongformerForMultiLabelSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                  gradient_checkpointing=False,\n",
    "                                                  attention_window = 512,\n",
    "                                                  num_labels = 6,\n",
    "                                                  cache_dir='/media/data_files/github/website_tutorials/data',\n",
    "                                                                     return_dict=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score \n",
    "# define optimizer\n",
    "learning_rate = 3e-5\n",
    "epochs = 3\n",
    "optimizer_ft =  AdamW([\n",
    "    {\"params\":model.parameters(),\n",
    "     \"lr\": learning_rate}])\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = get_linear_schedule_with_warmup(optimizer_ft,\n",
    "                                                   num_warmup_steps = 500, \n",
    "                                                   num_training_steps = len(dataloaders_dict['train']) * epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlealtru\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.15 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.14<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">longformer_multilabel_run</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/jlealtru/huggingface\" target=\"_blank\">https://wandb.ai/jlealtru/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/jlealtru/huggingface/runs/twa8bul6\" target=\"_blank\">https://wandb.ai/jlealtru/huggingface/runs/twa8bul6</a><br/>\n",
       "                Run data is saved locally in <code>/media/data_files/github/website_tutorials/notebooks/wandb/run-20210126_232016-twa8bul6</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm\n",
    "wandb.init(project='huggingface', name = 'longformer_multilabel_run')\n",
    "# define the training \n",
    "def train(model = None, train_dataloader = None, eval_dataloader = None, epochs = 3, \n",
    "          optimizer = None, scheduler = None, accumulation_steps = 8):\n",
    "    \n",
    "    wandb.watch(model, log=\"all\", log_freq=25)\n",
    "\n",
    "    best_eval_f1 = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        f1_scores = []\n",
    "        model.train()\n",
    "        \n",
    "        number_steps = 0\n",
    "        model.zero_grad()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            number_steps +=1\n",
    "            inputs = batch[0].squeeze(1).to(device)\n",
    "            att_mask = batch[1].squeeze(1).to(device)\n",
    "            labels = batch[2].to(device, dtype = torch.float)\n",
    "            ids = batch[3]\n",
    "            # feed into model\n",
    "            loss, logits = model(input_ids = inputs, attention_mask=att_mask, labels=labels)\n",
    "            #\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step+1) % accumulation_steps == 0:\n",
    "                optimizer.step()                            # Now we can do an optimizer step\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "            if number_steps%50==0:\n",
    "                sigmoid = torch.nn.Sigmoid()\n",
    "                probs = sigmoid(torch.Tensor(logits.cpu().data.numpy()))\n",
    "                preds = np.zeros(probs.shape)\n",
    "                preds[np.where(probs > 0.5)] = 1\n",
    "                labs_ = labels.cpu().data.numpy()\n",
    "                f1 = f1_score(labs_, preds, average='micro')\n",
    "                f1_scores.append(f1)\n",
    "                print(f' the current f1 score is {np.mean(f1_scores)}')\n",
    "        epoch_loss.append(loss)\n",
    "        print(f'the epoch loss was {np.sum(epoch_loss)} and'\n",
    "                  f'the f1 for epoch was {np.mean(f1_scores)}')\n",
    "                                \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        eval_steps = 0\n",
    "        eval_loss = []\n",
    "        f1_scores_val = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            eval_steps +=1\n",
    "            iterations_val+=1\n",
    "            inputs = batch[0].squeeze(1).to(device)\n",
    "            att_mask = batch[1].squeeze(1).to(device)\n",
    "            labels = batch[2].to(device, dtype = torch.float)\n",
    "            ids = batch[3]\n",
    "            # feed into model\n",
    "            loss, logits = model(input_ids = inputs, attention_mask=att_mask, labels=labels)\n",
    "            sig = torch.nn.Sigmoid()\n",
    "            probs = sigmoid(torch.Tensor(logits))\n",
    "            preds = np.zeros(probs.shape)\n",
    "            preds[np.where(probs > 0.5)] = 1\n",
    "            f1 = f1_score(labels.cpu().data.numpy(), preds, average='micro')\n",
    "            f1_scores_val.append(f1)\n",
    "            eval_loss.append(loss)\n",
    "            if counter_val % 10 == 0:\n",
    "                print(f'current validation loss is {np.sum(eval_loss)/eval_steps}'\n",
    "                     f'and current f1 score is {np.mean(f1_scores_val)}')\n",
    "    \n",
    "    eval_f1 = np.mean(val_accuracy)\n",
    "    if eval_f1 >= best_eval_f1:\n",
    "        best_eval_f1 = eval_f1\n",
    "        print(f'saving the model with f1 score of {best_eval_f1:,.2%}')\n",
    "        torch.save(model, '../results/')\n",
    "        \n",
    "    else:\n",
    "        print(f'model did not improve')\n",
    "                    \n",
    "    print('Training Completed!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the current f1 score is 0.12500000000000003\n",
      " the current f1 score is 0.125\n",
      " the current f1 score is 0.1346153846153846\n",
      " the current f1 score is 0.10096153846153846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dd28ce54e982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0meval_dataloader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       optimizer = optimizer_ft, scheduler = exp_lr_scheduler, accumulation_steps = 8)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-1fe56d578e44>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, eval_dataloader, epochs, optimizer, scheduler, accumulation_steps)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "train(model, train_dataloader = dataloaders_dict['train'], \n",
    "      eval_dataloader= dataloaders_dict['val'],\n",
    "      epochs = 3, \n",
    "      optimizer = optimizer_ft, scheduler = exp_lr_scheduler, accumulation_steps = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "talk about the data so we can decide the maximun length of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get info on how long are each one of the texts\n",
    "length_text_train = [len(x) for x in insults_train['comment_text']]\n",
    "length_text_test = [len(x) for x in insults_test['comment_text']]\n",
    "length_text = length_text_train+length_text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "hist_text_length = sns.histplot(data=length_text, binwidth=200, alpha=0.55, kde=True).set_title(\n",
    "    'Distribution of text length')\n",
    "hist_text_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mention there several observations longer than 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                           gradient_checkpointing=False,\n",
    "                                                           attention_window = 512,\n",
    "                                                           cache_dir='/media/data_files/github/website_tutorials/data')\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 2048)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from torch.nn import BCEWithLogitsLoss, Dropout, Linear\n",
    "from transformers import LongformerTokenizerFast, LongformerModel, LongformerConfig\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel,LongformerClassificationHead\n",
    "\n",
    "\n",
    "# instantiate the multi-label classification class\n",
    "\n",
    "class LongFormerMultilabelClass(LongformerPreTrainedModel):\n",
    "    def __init__(self, config, pos_weight = None):\n",
    "        super(LongFormerMultilabelClass, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.LongformerModel = LongformerModel(config)\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.classifier = Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids = None, attention_mask = None, token_type_ids = None, position_ids = None,\n",
    "                head_mask = None, inputs_embeds=None, labels = None):\n",
    "        \n",
    "        outputs = self.LongformerModel(input_ids, attention_mask=attention_mask, \n",
    "                                       token_type_ids=token_type_ids, position_ids=position_ids,\n",
    "                                       head_mask=head_mask,\n",
    "                                       inputs_embeds=inputs_embeds)\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
