{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label classification with the Longformer\n",
    "\n",
    "In a previous post I explored the functionality of the Longformer for text classification. In this post I will explore the performance of the Longformer in a setting of multilabel classification problem.\n",
    "\n",
    "For this dataset we need to download it manually from Kaggle and load it like we usually do with the datasets library. 'jigsaw_toxicity_pred', data_dir='/path/to/extracted/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import LongformerTokenizerFast, \\\n",
    "LongformerModel, LongformerConfig, Trainer, TrainingArguments,EvalPrediction, AutoTokenizer\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel, LongformerClassificationHead\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to instantiate a raw LongFormer Model and add a classifier head on top. \n",
    "\n",
    "talk about pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe\n",
    "insults = pd.read_csv('../data/jigsaw/train.csv')\n",
    "#insults = insults.iloc[0:6000]\n",
    "insults['labels'] = insults[insults.columns[2:]].values.tolist()\n",
    "insults = insults[['id','comment_text', 'labels']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "insults_train, insults_test = train_test_split(insults,\n",
    "                                               random_state = 55,\n",
    "                                               test_size = 0.35)\n",
    "insults_test.head()\n",
    "insults_test.columns\n",
    "'''\n",
    "train_size = 0.9\n",
    "train_dataset=insults.sample(frac=train_size,random_state=200)\n",
    "test_dataset=insults.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insults_test = pd.read_csv('../data/jigsaw/test.csv')\n",
    "#insults_test_ids = pd.read_csv('../data/jigsaw/test_labels.csv')\n",
    "#insults_test['labels']\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a class that will handle the data\n",
    "class Data_Processing(object):\n",
    "    def __init__(self, tokenizer, id_column, text_column, label_column):\n",
    "        \n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "    \n",
    "        # define the label column and transform it to list\n",
    "        self.label_column = label_column\n",
    "        \n",
    "        # define the id column and transform it to list\n",
    "        self.id_column = id_column.tolist()\n",
    "        \n",
    "    \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.text_column[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(comment_text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length= 1024,\n",
    "                                       padding = 'max_length',\n",
    "                                       return_attention_mask = True,\n",
    "                                       truncation = True,\n",
    "                                       return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        labels_ = torch.tensor(self.label_column[index], dtype=torch.float)\n",
    "        id_ = self.id_column[index]\n",
    "        return {'input_ids':input_ids[0], 'attention_mask':attention_mask[0], \n",
    "                'labels':labels_, 'id_':id_}\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.text_column) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "# create a class to process the traininga and test data\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', \n",
    "                                                    padding = 'max_length',\n",
    "                                                    truncation=True, \n",
    "                                                    max_length = 1024)\n",
    "training_data = Data_Processing(tokenizer, \n",
    "                                train_dataset['id'], \n",
    "                                train_dataset['comment_text'], \n",
    "                                train_dataset['labels'])\n",
    "\n",
    "test_data =  Data_Processing(tokenizer, \n",
    "                             test_dataset['id'], \n",
    "                             test_dataset['comment_text'], \n",
    "                             test_dataset['labels'])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'train': DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "                    'val': DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "                   }\n",
    "\n",
    "dataset_sizes = {'train':len(training_data),\n",
    "                 'val':len(test_data)\n",
    "                }\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check we are getting the right output\n",
    "a = next(iter(dataloaders_dict['val']))\n",
    "a['id_']\n",
    "#len(dataloaders_dict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Longformer for multilabel classification class\n",
    "\n",
    "class LongformerForMultiLabelSequenceClassification(LongformerPreTrainedModel):\n",
    "    \"\"\"\n",
    "    We instantiate a class of LongFormer adapted for a multilabel classification task. \n",
    "    This instance takes the pooled output of the LongFormer based model and passes it through a\n",
    "    classification head. We replace the traditional Cross Entropy loss with a BCE loss that generate probabilities\n",
    "    for all the labels that we feed into the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, pos_weight=None):\n",
    "        super(LongformerForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pos_weight = pos_weight\n",
    "        self.longformer = LongformerModel(config)\n",
    "        self.classifier = LongformerClassificationHead(config)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, global_attention_mask=None, \n",
    "                token_type_ids=None, position_ids=None, inputs_embeds=None, \n",
    "                labels=None):\n",
    "        \n",
    "        # create global attention on sequence, and a global attention token on the `s` token\n",
    "        # the equivalent of the CLS token on BERT models\n",
    "        if global_attention_mask is None:\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            global_attention_mask[:, 0] = 1\n",
    "        \n",
    "        # pass arguments to longformer model\n",
    "        outputs = self.longformer(\n",
    "            input_ids = input_ids,\n",
    "            attention_mask = attention_mask,\n",
    "            global_attention_mask = global_attention_mask,\n",
    "            token_type_ids = token_type_ids,\n",
    "            position_ids = position_ids)\n",
    "        \n",
    "        # if specified the model can return a dict where each key corresponds to the output of a\n",
    "        # LongformerPooler output class. In this case we take the last hidden state of the sequence\n",
    "        # which will have the shape (batch_size, sequence_length, hidden_size). \n",
    "        sequence_output = outputs['last_hidden_state']\n",
    "        \n",
    "        \n",
    "        # pass the hidden states through the classifier to obtain thee logits\n",
    "        logits = self.classifier(sequence_output)\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss(pos_weight=self.pos_weight)\n",
    "            labels = labels.float()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), \n",
    "                            labels.view(-1, self.num_labels))\n",
    "            #outputs = (loss,) + outputs\n",
    "            outputs = (loss,) + outputs\n",
    "        \n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LongformerForMultiLabelSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                  #'/media/data_files/github/website_tutorials/results/longformer_2048_multilabel_jigsaw',\n",
    "                                                  gradient_checkpointing=False,\n",
    "                                                  attention_window = 512,\n",
    "                                                  num_labels = 6,\n",
    "                                                  cache_dir='/media/data_files/github/website_tutorials/data',\n",
    "                                                                     return_dict=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "#acc = accuracy_score(labels, preds)\n",
    "    #acc = accuracy_score(labels, preds)\n",
    "    \n",
    "def multi_label_metric(\n",
    "    predictions, \n",
    "    references, \n",
    "    ):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_true = references\n",
    "    y_pred[np.where(probs >= 0.5)] = 1\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    metrics = {'f1':f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    result = multi_label_metric(\n",
    "        predictions=preds, \n",
    "        references=p.label_ids\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '/media/data_files/github/website_tutorials/results',\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 32,    \n",
    "    per_device_eval_batch_size= 32,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps = 2000,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 4,\n",
    "    fp16 = False,\n",
    "    logging_dir='/media/data_files/github/website_tutorials/logs',\n",
    "    dataloader_num_workers = 0,\n",
    "    run_name = 'longformer_multilabel_paper_trainer_1048'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer class and check for available devices\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data,\n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics = compute_metrics,\n",
    "    #data_collator = Data_Processing(),\n",
    "\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insults_test = pd.read_csv('../data/jigsaw/test.csv')\n",
    "#insults_test = insults_test.iloc[0:101]\n",
    "insults_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a class that will handle the data\n",
    "class Data_Processing_test():\n",
    "    def __init__(self, tokenizer, id_column, text_column):\n",
    "        \n",
    "        # define the text column from the dataframe\n",
    "        self.text_column = text_column.tolist()\n",
    "                    \n",
    "        # define the id column and transform it to list\n",
    "        self.id_column = id_column.tolist()\n",
    "            \n",
    "# iter method to get each element at the time and tokenize it using bert        \n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.text_column[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(comment_text,\n",
    "                                       add_special_tokens = True,\n",
    "                                       max_length= 1024,\n",
    "                                       padding = 'max_length',\n",
    "                                       return_attention_mask = True,\n",
    "                                       truncation = True,\n",
    "                                       return_tensors='pt')\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        id_ = self.id_column[index]\n",
    "        return {'input_ids':input_ids[0], 'attention_mask':attention_mask[0], \n",
    "                'id_':id_}\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.text_column) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# create a class to process the traininga and test data\n",
    "\n",
    "test_data_pred =  Data_Processing_test(tokenizer,\n",
    "                                       insults_test['id'], \n",
    "                                       insults_test['comment_text'])\n",
    "\n",
    "# use the dataloaders class to load the data\n",
    "dataloaders_dict = {'test': DataLoader(test_data_pred,\n",
    "                                                 batch_size=batch_size, shuffle=True, num_workers=2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction():\n",
    "    prediction_data_frame_list = []\n",
    "    with torch.no_grad():\n",
    "        trainer.model.eval()\n",
    "        for i, batch in enumerate(dataloaders_dict['test']):\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            # feed the sequences to the model, specifying the attention mask\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "            # feed the logits returned by the model to the softmax to classify the function\n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            probs = sigmoid(torch.Tensor(outputs[0].detach().cpu().data.numpy()))\n",
    "            #probs.\n",
    "            probs = np.array(probs)\n",
    "            #print(np.array([[i] for i in probs]))\n",
    "            y_pred = np.zeros(probs.shape)\n",
    "            y_pred = probs\n",
    "            temp_data = pd.DataFrame(zip(batch['id_'], probs), columns = ['id', 'target'\n",
    "                                                                         ])\n",
    "            #print(temp_data)\n",
    "            prediction_data_frame_list.append(temp_data)                \n",
    "\n",
    "    prediction_df = pd.concat(prediction_data_frame_list)\n",
    "    prediction_df[['toxic','severe_toxic',\n",
    "                   'obscene','threat','insult','identity_hate']] = pd.DataFrame(prediction_df.target.tolist(),\n",
    "                                                                                index= prediction_df.index)\n",
    "    prediction_df = prediction_df.drop(columns = 'target')\n",
    "    return prediction_df\n",
    "\n",
    "predictions = prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('../data/jigsaw/submission_longf_1024.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained('/media/data_files/github/website_tutorials/results/longformer_base_multilabel_1024')\n",
    "tokenizer.save_pretrained('/media/data_files/github/website_tutorials/results/longforner_base_multilabel_1024')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mention there several observations longer than 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
    "                                                           gradient_checkpointing=False,\n",
    "                                                           attention_window = 512,\n",
    "                                                           cache_dir='/media/data_files/github/website_tutorials/data')\n",
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-base-4096', max_length = 2048)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from torch.nn import BCEWithLogitsLoss, Dropout, Linear\n",
    "from transformers import LongformerTokenizerFast, LongformerModel, LongformerConfig\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel,LongformerClassificationHead\n",
    "\n",
    "\n",
    "# instantiate the multi-label classification class\n",
    "\n",
    "class LongFormerMultilabelClass(LongformerPreTrainedModel):\n",
    "    def __init__(self, config, pos_weight = None):\n",
    "        super(LongFormerMultilabelClass, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.LongformerModel = LongformerModel(config)\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.classifier = Linear(config.hidden_size, config.num_labels)\n",
    "        self.init_weights()\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids = None, attention_mask = None, token_type_ids = None, position_ids = None,\n",
    "                head_mask = None, inputs_embeds=None, labels = None):\n",
    "        \n",
    "        outputs = self.LongformerModel(input_ids, attention_mask=attention_mask, \n",
    "                                       token_type_ids=token_type_ids, position_ids=position_ids,\n",
    "                                       head_mask=head_mask,\n",
    "                                       inputs_embeds=inputs_embeds)\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
